import cv2
import numpy as np
import pickle
import time
import os
import scipy.spatial.distance
import threading
from flask import Flask, render_template, Response

# Import the model loading and preprocessing functions
from facenet_model import load_facenet_model, preprocess_image_for_facenet

# --- Configuration ---
ENCODINGS_FILE = 'encodings_tf.pkl' # Path to the file generated by encode_faces_tf.py
PROTOTXT_PATH = 'models/deploy.prototxt.txt'
CAFFEMODEL_PATH = 'models/res10_300x300_ssd_iter_140000.caffemodel'

# Cosine distance threshold for recognition. Lower means stricter match.
# Tune this based on your data and model performance.
RECOGNITION_THRESHOLD = 0.5

# Minimum confidence threshold for the face detector
DETECTION_CONFIDENCE_THRESHOLD = 0.7

# Scale factor for resizing the frame *before* detection to speed it up.
# Lower values are faster but might miss smaller faces.
DETECTION_SCALE_FACTOR = 0.5 # Detect on a scaled frame

# How many frames to skip between *embedding* calculations (to save CPU/GPU)
# Face detection is done on every frame.
EMBEDDING_FRAME_INTERVAL = 3

# --- Global Variables / Shared State ---
# These variables will be shared between the Flask thread and the video processing thread
latest_frame = None # Stores the latest processed frame (with drawings) as a NumPy array
frame_lock = threading.Lock() # Lock to ensure safe access to latest_frame
video_capture = None # OpenCV VideoCapture object
model = None # FaceNet Model
detector = None # DNN Face Detector
known_face_encodings_np = None # Known face embeddings as a NumPy array
known_face_names = None # List of known names

# --- Flask App Setup ---
app = Flask(__name__)

# --- Background Video Processing Thread ---
def process_video():
    global latest_frame, video_capture, model, detector, known_face_encodings_np, known_face_names

    print("Starting video processing thread...")

    # --- Load Resources ---
    # Load encodings
    print(f"Loading face encodings from {ENCODINGS_FILE}...")
    try:
        with open(ENCODINGS_FILE, 'rb') as f:
            data = pickle.load(f)
            known_face_encodings = data["encodings"]
            known_face_names = data["names"]
        known_face_encodings_np = np.array(known_face_encodings)
        print(f"Loaded {len(known_face_encodings)} known face encodings for {len(set(known_face_names))} unique individuals.")
    except FileNotFoundError:
        print(f"Error: {ENCODINGS_FILE} not found.")
        # Consider exiting or signaling error state
        return
    except Exception as e:
        print(f"Error loading encodings file: {e}")
        return

    # Load FaceNet model
    model = load_facenet_model()
    if model is None:
        print("Failed to load the FaceNet model.")
        return

    # Load DNN face detector
    print(f"Loading DNN face detector...")
    try:
        detector = cv2.dnn.readNetFromCaffe(PROTOTXT_PATH, CAFFEMODEL_PATH)
        print("DNN face detector loaded successfully.")
    except FileNotFoundError:
         print(f"Error: DNN detector files not found.")
         print(f"Please download '{os.path.basename(PROTOTXT_PATH)}' and '{os.path.basename(CAFFEMODEL_PATH)}' and place them in the '{os.path.dirname(PROTOTXT_PATH)}' folder.")
         return
    except Exception as e:
        print(f"Error loading DNN face detector: {e}")
        return

    # Initialize video capture
    video_capture = cv2.VideoCapture(0) # 0 is typically the default webcam

    if not video_capture.isOpened():
        print("Error: Could not open webcam.")
        # Consider signaling error state
        return

    print("Resources loaded. Starting webcam feed processing loop.")

    # Initialize variables for the loop
    embedding_processed_frame_counter = 0 # Counter to control embedding calculation frequency
    last_face_locations = [] # Store locations from last detection pass
    last_face_names = []     # Store predicted names from last recognition pass

    while True:
        ret, frame = video_capture.read()

        if not ret:
            print("Error reading frame from webcam. Stopping thread.")
            break

        embedding_processed_frame_counter += 1

        # Get frame dimensions
        (h, w) = frame.shape[:2]

        # Create a blob from the frame for the DNN detector
        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,
                                     (300, 300), (104.0, 177.0, 123.0))

        detector.setInput(blob)
        detections = detector.forward()

        current_frame_face_locations = []
        current_frame_face_embeddings = [] # To store embeddings if calculated this frame

        # Loop over the detections
        for i in range(0, detections.shape[2]):
            confidence = detections[0, 0, i, 2]

            if confidence > DETECTION_CONFIDENCE_THRESHOLD:
                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])
                (startX, startY, endX, endY) = box.astype("int")

                startX = max(0, startX)
                startY = max(0, startY)
                endX = min(w, endX)
                endY = min(h, endY)

                # Store the location (startY, endX, endY, startX)
                current_frame_face_locations.append((startY, endX, endY, startX))

                # --- Conditional Recognition Logic ---
                if embedding_processed_frame_counter % EMBEDDING_FRAME_INTERVAL == 0:
                     # Extract the face ROI
                     face_roi = frame[startY:endY, startX:endX]

                     # Ensure ROI is not empty
                     if face_roi.size == 0:
                         current_frame_face_embeddings.append(None) # Append None if ROI is empty
                         continue

                     try:
                         # Convert BGR to RGB and preprocess
                         face_roi_rgb = cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB)
                         processed_face = preprocess_image_for_facenet(face_roi_rgb)

                         # Get the face embedding using the keras-facenet model
                         embedding = model.embeddings(processed_face)[0]
                         current_frame_face_embeddings.append(embedding)

                     except Exception as e:
                        current_frame_face_embeddings.append(None) # Append None if processing fails
                        pass # Continue to next detected face


        # --- Perform Recognition and Update State ---
        if embedding_processed_frame_counter % EMBEDDING_FRAME_INTERVAL == 0 and len(current_frame_face_embeddings) > 0 and not all(e is None for e in current_frame_face_embeddings):
            # Filter out None embeddings and corresponding locations
            successful_embeddings = [e for e in current_frame_face_embeddings if e is not None]
            successful_locations = [loc for loc, emb in zip(current_frame_face_locations, current_frame_face_embeddings) if emb is not None]

            if successful_embeddings: # Check if there are any successful embeddings left
                face_embeddings_np = np.array(successful_embeddings)

                # Calculate distances
                # Use cdist for efficient pairwise distance calculation between current faces and known faces
                distances = scipy.spatial.distance.cdist(face_embeddings_np, known_face_encodings_np, metric='cosine')

                # Find best matches and assign names
                current_frame_face_names = []
                for i in range(distances.shape[0]):
                    min_dist_index = np.argmin(distances[i])
                    min_distance = distances[i, min_dist_index]

                    if min_distance < RECOGNITION_THRESHOLD:
                        name = known_face_names[min_dist_index]
                    else:
                        name = "Unknown"

                    current_frame_face_names.append(name)

                # Update the state for drawing
                last_face_locations = successful_locations
                last_face_names = current_frame_face_names
            else:
                 # If no embeddings were successful this frame, clear last known state
                 last_face_locations = []
                 last_face_names = []

        # Reset embedding counter
        if embedding_processed_frame_counter >= EMBEDDING_FRAME_INTERVAL:
            embedding_processed_frame_counter = 0


        # --- Draw Results ---
        # Draw based on the last successful recognition results (or empty if none)
        for (top, right, bottom, left), name in zip(last_face_locations, last_face_names):
            color = (0, 255, 0) if name != "Unknown" else (0, 0, 255) # Green/Red box
            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
            cv2.rectangle(frame, (left, bottom - 30), (right, bottom), color, cv2.FILLED)
            font = cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.6, (255, 255, 255), 1)

        # --- Make the frame available to the Flask app ---
        # Acquire the lock, update the frame, release the lock
        with frame_lock:
            latest_frame = frame.copy() # Use .copy() to avoid issues if OpenCV modifies the frame later

    # --- Cleanup ---
    if video_capture:
        video_capture.release()
    print("Video processing thread stopped.")


# --- Flask Routes ---

@app.route('/')
def index():
    """Video streaming home page."""
    return render_template('index.html')

def generate_frames():
    """Generates JPEG frames from the video feed."""
    print("Generating frames for video feed...")
    while True:
        # Acquire the lock to access the latest frame
        with frame_lock:
            # Check if a frame is available
            if latest_frame is None:
                continue # No frame yet, try again

            # Encode the frame as JPEG
            # imencode returns (success, buffer)
            success, buffer = cv2.imencode('.jpg', latest_frame)

            if not success:
                print("Error encoding frame.")
                continue

            # Convert buffer to bytes
            frame_bytes = buffer.tobytes()

        # Yield the frame in the Motion JPEG format
        # This consists of a boundary string followed by the JPEG data
        # Need two dashes before the boundary in the first line
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

        # Optional: Add a small sleep if you want to limit the stream FPS
        # time.sleep(0.01) # e.g., ~100 FPS limit

@app.route('/video_feed')
def video_feed():
    """Video streaming route. Streams MJPEG responses."""
    # Return the response generated by the generate_frames function
    # Set the content type to multipart/x-mixed-replace with a boundary
    return Response(generate_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')


# --- Main Execution ---
if __name__ == '__main__':
    print("Starting Flask app...")
    # Start the background video processing thread
    video_thread = threading.Thread(target=process_video)
    video_thread.daemon = True # Allow the main thread to exit even if this thread is running
    video_thread.start()

    # Give the thread a moment to load resources
    time.sleep(2.0) # Adjust if needed

    # Run the Flask app
    # Use threaded=True (default in recent Flask) or processes=... for concurrency
    # Use debug=True for development (reloads code on change)
    app.run(host='0.0.0.0', port=5000, threaded=True, debug=False)

    # Cleanup: Stop the video processing thread gracefully
    # This is more complex than needed for a simple Ctrl+C shutdown.
    # For Ctrl+C, the daemon=True setting helps.
    print("Flask app stopped. Waiting for video thread (if not daemon).")
    # If daemon=False, you might need video_thread.join() here,
    # but then you need a way to signal the thread to exit its loop.
    # Daemon=True is simpler for quick examples.