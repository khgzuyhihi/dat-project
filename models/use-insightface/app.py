import cv2
import numpy as np
import pickle
import time
import os
import threading
from flask import Flask, render_template, Response
from scipy.spatial.distance import cosine, cdist # cdist for pairwise distance

# InsightFace imports
import insightface
import insightface.app as app

# --- Configuration ---
ENCODINGS_FILE = 'encodings_if.pkl' # Path to the file generated by encode_faces_if.py
INSIGHTFACE_MODEL_NAME = 'buffalo_l' # Same model bundle name as encoding step
# Context ID for model inference (-1 for CPU, 0 for GPU 0, etc.)
# Use 0 if you have a GPU and onnxruntime-gpu installed for better real-time performance.
# Use -1 for CPU.
CTX_ID = -1 # <--- **CHANGE THIS TO 0 IF YOU HAVE A GPU**

# Cosine distance threshold for recognition. Lower means stricter match.
# You'll need to tune this based on your data and model performance.
# Typical values for InsightFace embeddings are around 0.4 to 0.6.
RECOGNITION_THRESHOLD = 0.5 # <--- TUNE THIS VALUE

# Minimum confidence threshold for detected faces to be considered
# InsightFace's detector has its own internal threshold, but you can filter results further.
# Lower values might include more false positives.
DETECTION_FILTER_THRESHOLD = 0.7 # <--- TUNE THIS VALUE (Can use face.det_score)

# How many frames to skip between *embedding* calculations (to save CPU/GPU)
# Face detection is usually done on every frame by InsightFace's get() method.
EMBEDDING_FRAME_INTERVAL = 3

# --- Global Variables / Shared State ---
latest_frame = None # Stores the latest processed frame (with drawings) as a NumPy array
frame_lock = threading.Lock() # Lock to ensure safe access to latest_frame
video_capture = None # OpenCV VideoCapture object
face_rec_app = None # InsightFace FaceAnalysis app/model

known_face_encodings_np = None # Known face embeddings as a NumPy array
known_face_names = None # List of known names

# Store results from the last recognition pass for drawing
last_face_locations = []
last_face_names = []


# --- Flask App Setup ---
app = Flask(__name__)

# --- Background Video Processing Thread ---
def process_video():
    global latest_frame, video_capture, face_rec_app
    global known_face_encodings_np, known_face_names
    global last_face_locations, last_face_names

    print("Starting video processing thread...")

    # --- Load Resources ---
    # Load encodings
    print(f"Loading face encodings from {ENCODINGS_FILE}...")
    try:
        with open(ENCODINGS_FILE, 'rb') as f:
            data = pickle.load(f)
            known_face_encodings_np = data["encodings"] # Already loaded as numpy array
            known_face_names = data["names"]
        print(f"Loaded {known_face_encodings_np.shape[0]} known face encodings for {len(set(known_face_names))} unique individuals.")
    except FileNotFoundError:
        print(f"Error: {ENCODINGS_FILE} not found.")
        # Consider exiting or signaling error state
        return
    except Exception as e:
        print(f"Error loading encodings file: {e}")
        return

    # Load InsightFace model
    print(f"Loading InsightFace model '{INSIGHTFACE_MODEL_NAME}' for inference...")
    try:
        face_rec_app = app.FaceAnalysis(name=INSIGHTFACE_MODEL_NAME, root='~/.insightface')
        # Prepare the model. Use a detection size suitable for real-time frames.
        # Smaller sizes are faster but might miss small faces.
        face_rec_app.prepare(ctx_id=CTX_ID, det_size=(320, 320)) # Or (640, 640) etc.
        print("InsightFace model loaded and prepared.")
    except Exception as e:
        print(f"Error loading or preparing InsightFace model: {e}")
        print("Ensure InsightFace models are downloaded or accessible.")
        print("Check ONNX Runtime and GPU setup (if using CTX_ID >= 0).")
        return

    # Initialize video capture
    video_capture = cv2.VideoCapture(0) # 0 is typically the default webcam

    if not video_capture.isOpened():
        print("Error: Could not open webcam.")
        # Consider signaling error state
        return

    print("Resources loaded. Starting webcam feed processing loop.")

    # Initialize variables for the loop
    embedding_processed_frame_counter = 0 # Counter to control embedding calculation frequency

    while True:
        ret, frame = video_capture.read()

        if not ret:
            print("Error reading frame from webcam. Stopping thread.")
            break

        embedding_processed_frame_counter += 1

        # --- Face Detection and Embedding (InsightFace get() method handles both) ---
        # InsightFace's get() expects RGB image, convert from OpenCV's BGR
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # The get() method performs detection and computes embeddings for ALL detected faces
        # Returns a list of Face objects
        try:
            faces = face_rec_app.get(rgb_frame)
        except Exception as e:
            print(f"Error during InsightFace get(): {e}")
            faces = [] # Continue with no faces if error occurs

        current_frame_face_locations = []
        current_frame_face_names = [] # Only populated on recognition frames

        # Filter faces by detection score (optional, InsightFace has internal thresholds too)
        faces = [face for face in faces if face.det_score >= DETECTION_FILTER_THRESHOLD]

        # --- Perform Recognition (conditionally) ---
        # Store embeddings and locations for current frame detections regardless of recognition interval
        current_frame_embeddings = []
        current_frame_locations = []

        for face in faces:
            # Get bounding box (InsightFace uses [left, top, right, bottom])
            bbox = face.bbox.astype(int)
            (left, top, right, bottom) = bbox
            # Store in (top, right, bottom, left) format for consistency if desired,
            # but drawing uses (left, top), (right, bottom)
            current_frame_locations.append((left, top, right, bottom))
            current_frame_embeddings.append(face.embedding)


        # Only perform embedding comparison and name assignment every N frames
        if embedding_processed_frame_counter % EMBEDDING_FRAME_INTERVAL == 0:
            if len(current_frame_embeddings) > 0:
                 current_frame_embeddings_np = np.array(current_frame_embeddings)

                 # Calculate distances between current frame face embeddings and known embeddings
                 # cdist(XA, XB, metric='cosine') computes distances between each row of XA and each row of XB
                 # XA = current frame embeddings, XB = known embeddings
                 # Result shape is (num_current_faces, num_known_faces)
                 distances = cdist(current_frame_embeddings_np, known_face_encodings_np, metric='cosine')

                 # For each detected face, find the closest known face
                 temp_names = []
                 for i in range(distances.shape[0]):
                     min_dist_index = np.argmin(distances[i])
                     min_distance = distances[i, min_dist_index]

                     # Check if the minimum distance is below the threshold
                     if min_distance < RECOGNITION_THRESHOLD:
                         name = known_face_names[min_dist_index]
                     else:
                         name = "Unknown" # Distance is too large, consider it unknown

                     temp_names.append(name)

                 # Update the state for drawing with results from THIS recognition pass
                 last_face_locations = current_frame_locations # Use locations from this frame's detection
                 last_face_names = temp_names

            else: # No faces detected in this recognition frame
                 last_face_locations = []
                 last_face_names = []


        # Reset embedding counter
        if embedding_processed_frame_counter >= EMBEDDING_FRAME_INTERVAL:
            embedding_processed_frame_counter = 0


        # --- Draw Results ---
        # Draw based on the last successful recognition results (or empty if none)
        # Use the locations and names stored in last_face_locations and last_face_names
        for (left, top, right, bottom), name in zip(last_face_locations, last_face_names):
            color = (0, 255, 0) if name != "Unknown" else (0, 0, 255) # Green/Red box
            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)
            # Draw a label with a name below the face
            cv2.rectangle(frame, (left, bottom - 30), (right, bottom), color, cv2.FILLED)
            font = cv2.FONT_HERSHEY_DUPLEX
            cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.6, (255, 255, 255), 1)


        # --- Make the frame available to the Flask app ---
        with frame_lock:
            latest_frame = frame.copy() # Use .copy() to avoid issues


    # --- Cleanup ---
    if video_capture:
        video_capture.release()
    print("Video processing thread stopped.")


# --- Flask Routes ---

@app.route('/')
def index():
    """Video streaming home page."""
    return render_template('index.html')

def generate_frames():
    """Generates JPEG frames from the video feed."""
    #print("Generating frames for video feed...") # Avoid spamming console
    while True:
        # Acquire the lock to access the latest frame
        with frame_lock:
            # Check if a frame is available
            if latest_frame is None:
                continue # No frame yet, try again

            # Encode the frame as JPEG
            success, buffer = cv2.imencode('.jpg', latest_frame)

            if not success:
                # print("Error encoding frame.") # Avoid spamming console
                continue

            # Convert buffer to bytes
            frame_bytes = buffer.tobytes()

        # Yield the frame in the Motion JPEG format
        yield (b'--frame\r\n'
               b'Content-Type: image/jpeg\r\n\r\n' + frame_bytes + b'\r\n')

        # Optional: Add a small sleep if you want to limit the stream FPS
        # time.sleep(0.01) # e.g., ~100 FPS limit

@app.route('/video_feed')
def video_feed():
    """Video streaming route. Streams MJPEG responses."""
    return Response(generate_frames(),
                    mimetype='multipart/x-mixed-replace; boundary=frame')


# --- Main Execution ---
if __name__ == '__main__':
    print("Starting Flask app...")
    # Start the background video processing thread
    video_thread = threading.Thread(target=process_video)
    video_thread.daemon = True # Allow the main thread to exit even if this thread is running
    video_thread.start()

    # Give the thread a moment to load resources and start capturing
    time.sleep(3.0) # Adjust if needed

    # Run the Flask app
    # Use threaded=True (default in recent Flask) or processes=... for concurrency
    # Use debug=True for development (reloads code on change, but not thread-safe)
    # Set debug=False for production/reliable threading
    app.run(host='0.0.0.0', port=5000, threaded=True, debug=False)

    print("Flask app stopped.")